"""ä½¿ç”¨çŸ¥è¯†è’¸é¦è®­ç»ƒå­¦ç”Ÿæ¨¡åž‹"""
import torch
import torchvision
from torch import optim
from torchvision.transforms import transforms
from tqdm import tqdm

from utils.criterion import test_model
from utils.distill_loss import DistillationLoss
from utils.micro_ghost_resnet import MicroResNetGhost
from utils.persist import persist_learning_history
from utils.res_net import ResNet20


def train_microresnet_ghost_with_distillation():
    print("ðŸš€ å¼€å§‹è®­ç»ƒå­¦ç”Ÿæ¨¡åž‹ MicroResNetGhostï¼ˆå¸¦çŸ¥è¯†è’¸é¦ï¼‰...")

    model_path = "../new_model_weights/microresnet_ghost_cifar10_distill_best.pth"

    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),  # éšæœºè£å‰ª
        transforms.RandomHorizontalFlip(),      # éšæœºæ°´å¹³ç¿»è½¬
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # CIFAR10å®žé™…å‡å€¼æ–¹å·®
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    # ä¸‹è½½æ•°æ®é›†
    trainset = torchvision.datasets.CIFAR10(
        root='../data', train=True, download=True, transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root='../data', train=False, download=True, transform=transform_test
    )

    # æ•°æ®åŠ è½½å™¨
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)


    # æ¨¡åž‹å’Œä¼˜åŒ–å™¨
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    teacher_model = ResNet20(num_classes=10).to(device)
    student_model = MicroResNetGhost(num_classes=10).to(device)

    # åŠ è½½é¢„è®­ç»ƒçš„æ•™å¸ˆæ¨¡åž‹
    teacher_model.load_state_dict(torch.load("../new_model_weights/resnet20_cifar10_best.pth"))
    teacher_model.eval()
    for param in teacher_model.parameters():
        param.requires_grad = False

    optimizer = optim.SGD(
        student_model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4
    )
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 120, 160], gamma=0.1)
    criterion = DistillationLoss(alpha=0.7, temperature=4)

    # è®­ç»ƒè®°å½•
    best_acc = 0
    loss_history = []

    # è®­ç»ƒå¾ªçŽ¯
    for epoch in range(200):
        student_model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(trainloader, desc=f"Student KD Epoch {epoch + 1}/200")
        for batch_idx, (inputs, targets) in enumerate(pbar):
            inputs, targets = inputs.to(device), targets.to(device)

            optimizer.zero_grad()

            # å­¦ç”Ÿæ¨¡åž‹è¾“å‡º
            student_outputs = student_model(inputs)

            # æ•™å¸ˆæ¨¡åž‹è¾“å‡ºï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
            with torch.no_grad():
                teacher_outputs = teacher_model(inputs)

            # è®¡ç®—è’¸é¦æŸå¤±
            loss = criterion(student_outputs, teacher_outputs, targets)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = student_outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            pbar.set_postfix({
                "Loss": f"{loss.item():.3f}",
                "Acc": f"{100.*correct/total:.2f}%"
            })

        # è®°å½•æŸå¤±
        loss_history.append(running_loss / len(trainloader))

        # æ¯10ä¸ªepochæµ‹è¯•ä¸€æ¬¡
        if (epoch + 1) % 10 == 0 or epoch == 0:
            test_acc = test_model(student_model, testloader, device)
            print(f"Epoch {epoch+1}: Test Accuracy = {test_acc:.2f}%")

            if test_acc > best_acc:
                best_acc = test_acc
                torch.save(student_model.state_dict(), model_path)
                print(f"âœ… æ–°çš„æœ€ä½³å‡†ç¡®çŽ‡: {best_acc:.2f}%")

        scheduler.step()

    persist_learning_history(loss_history, "microresnet_ghost_loss")


if __name__ == '__main__':
    train_microresnet_ghost_with_distillation()
